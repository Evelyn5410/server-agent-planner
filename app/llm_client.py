import os
import json
from google import genai
from google.cloud import secretmanager

# Mock mode for local testing without GCP credentials
MOCK_LLM = os.getenv("MOCK_LLM", "false").lower() == "true"

from app.constants import MODEL  # Use centralized MODEL from constants


class MockUsageMetadata(dict):
    """Mock usage metadata for local testing (JSON serializable)."""
    def __init__(self):
        super().__init__({
            "prompt_token_count": 100,
            "candidates_token_count": 50,
            "total_token_count": 150
        })
        self.prompt_token_count = 100
        self.candidates_token_count = 50
        self.total_token_count = 150


class MockResponse:
    """Mock LLM response for local testing."""
    def __init__(self, text: str):
        self._text = text
        self.usage_metadata = MockUsageMetadata()

    @property
    def text(self):
        return self._text


class MockModel:
    """Mock model that returns predefined responses based on prompt content."""

    def generate_content(self, model, contents, config=None):
        # Extract text from new contents format: [{"role": "user", "parts": [{"text": "..."}]}]
        content_text = ""
        for msg in contents:
            parts = msg.get("parts", [])
            for part in parts:
                if isinstance(part, dict) and "text" in part:
                    content_text += part["text"]

        # Planner response
        if "PLANNING module" in content_text:
            response = json.dumps({
                "task": "Process user request",
                "status": "approved",
                "constraints": ["Must be clear and concise", "Must be accurate"],
                "assumptions": ["User wants a helpful response"],
                "output_requirements": ["Provide a complete answer"]
            })
            return MockResponse(response)

        # Generator response
        if "GENERATION module" in content_text:
            return MockResponse(
                "[Mock Generated Output]\n\n"
                "This is a mock response for local testing.\n"
                "In production, this would be generated by Gemini based on the plan."
            )

        # Extractor response
        if "EXTRACTION module" in content_text:
            response = json.dumps({
                "extracted_rules": [
                    {
                        "type": "requirement",
                        "statement": "Mock extracted rule from document chunk",
                        "confidence": "high"
                    }
                ]
            })
            return MockResponse(response)

        # Default response
        return MockResponse("Mock response for unrecognized prompt type.")


class MockClient:
    """Mock client that mimics the genai.Client interface."""
    def __init__(self):
        self.models = MockModel()

def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")

if MOCK_LLM:
    print("[LLM Client] Running in MOCK mode - no GCP credentials required")
    client = MockClient()
else:

    # Configuration from environment variables (set via GitHub secrets during deployment)
    GCP_PROJECT = "toolhub-web"
    GCP_LOCATION = os.getenv("GCP_LOCATION", "us-central1")
    # API key from Google Cloud Secret Manager "v_api_key" - accessed at runtime in Cloud Run
    # VERTEX_API_KEY = os.getenv("VERTEX_API_KEY")
    VERTEX_API_KEY= get_secret(GCP_PROJECT, "v_api_key")

    if not GCP_PROJECT:
        raise RuntimeError("GCP_PROJECT_ID environment variable must be set")

    # Debug: log which auth method is being used
    print(f"[LLM Client] GCP_PROJECT: {GCP_PROJECT}")
    print(f"[LLM Client] GCP_LOCATION: {GCP_LOCATION}")
    print(f"[LLM Client] VERTEX_API_KEY set: {bool(VERTEX_API_KEY)}")
    if VERTEX_API_KEY:
        print(f"[LLM Client] VERTEX_API_KEY length: {len(VERTEX_API_KEY)}")
        print(f"[LLM Client] VERTEX_API_KEY prefix: {VERTEX_API_KEY[:10]}...")

    # Vertex AI mode: use service account credentials (recommended for Cloud Run)
    # This uses the Cloud Run service account automatically
    print("[LLM Client] Using Vertex AI with service account credentials")
    client = genai.Client(
        vertexai=True,
        project=GCP_PROJECT,
        location=GCP_LOCATION,
    )

  
