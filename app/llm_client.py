import os
import json

# Mock mode for local testing without GCP credentials
MOCK_LLM = os.getenv("MOCK_LLM", "false").lower() == "true"

MODEL = "gemini-1.5-flash"


class MockUsageMetadata(dict):
    """Mock usage metadata for local testing (JSON serializable)."""
    def __init__(self):
        super().__init__({
            "prompt_token_count": 100,
            "candidates_token_count": 50,
            "total_token_count": 150
        })
        self.prompt_token_count = 100
        self.candidates_token_count = 50
        self.total_token_count = 150


class MockResponse:
    """Mock LLM response for local testing."""
    def __init__(self, text: str):
        self._text = text
        self.usage_metadata = MockUsageMetadata()

    @property
    def text(self):
        return self._text


class MockModel:
    """Mock model that returns predefined responses based on prompt content."""

    def generate_content(self, model, contents, generation_config=None):
        # Determine response type based on system prompt content
        system_content = ""
        user_content = ""

        for msg in contents:
            role = msg.get("role", "")
            parts = msg.get("parts", [])
            text = parts[0] if parts else ""
            if role == "system":
                system_content = text
            elif role == "user":
                user_content = text

        # Planner response
        if "PLANNING module" in system_content:
            response = json.dumps({
                "task": f"Process user request: {user_content[:50]}...",
                "status": "approved",
                "constraints": ["Must be clear and concise", "Must be accurate"],
                "assumptions": ["User wants a helpful response"],
                "output_requirements": ["Provide a complete answer"]
            })
            return MockResponse(response)

        # Generator response
        if "GENERATION module" in system_content:
            return MockResponse(
                f"[Mock Generated Output]\n\n"
                f"This is a mock response for local testing.\n"
                f"In production, this would be generated by Gemini based on the plan."
            )

        # Extractor response
        if "EXTRACTION module" in system_content:
            response = json.dumps({
                "extracted_rules": [
                    {
                        "type": "requirement",
                        "statement": "Mock extracted rule from document chunk",
                        "confidence": "high"
                    }
                ]
            })
            return MockResponse(response)

        # Default response
        return MockResponse("Mock response for unrecognized prompt type.")


class MockClient:
    """Mock client that mimics the genai.Client interface."""
    def __init__(self):
        self.models = MockModel()


if MOCK_LLM:
    print("[LLM Client] Running in MOCK mode - no GCP credentials required")
    client = MockClient()
else:
    from google import genai

    # Configuration from environment variables (set via GitHub secrets during deployment)
    GCP_PROJECT = os.getenv("GCP_PROJECT_ID")
    GCP_LOCATION = os.getenv("GCP_LOCATION", "us-central1")
    # API key from Google Cloud Secret Manager "v_api_key" - accessed at runtime in Cloud Run
    VERTEX_API_KEY = os.getenv("VERTEX_API_KEY")

    if not GCP_PROJECT:
        raise RuntimeError("GCP_PROJECT_ID environment variable must be set")

    # Use Vertex AI with project credentials (preferred in Cloud Run with service account)
    # or API key if provided
    if VERTEX_API_KEY:
        client = genai.Client(api_key=VERTEX_API_KEY)
    else:
        # Uses default credentials (Workload Identity in Cloud Run)
        client = genai.Client(
            vertexai=True,
            project=GCP_PROJECT,
            location=GCP_LOCATION,
        )
